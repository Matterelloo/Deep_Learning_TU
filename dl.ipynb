{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning: CartPole with DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gymnasium\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Agent\n",
    "\n",
    "DQN (Deep Q-Network) that inherits from `torch.nn` module. A simple feed-forward network with two hidden layers of size `hidden_size`. Takes the environment state (a vector of length `input_size`) and outputs **Q-values** ffor each of the `output_size` actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        return self.out(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReplayBuffer\n",
    "\n",
    "A datastructure (buffer) to efficiently store experiences, that we may sample a tuple containing `(state, action, reward, next_state, done)` from, get the length of the buffer and add batches to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"A simple replay buffer to store experiences\"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "    # args is a tuple of (state, action, reward, next_state, done)\n",
    "    def push(self, *args):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = args\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    # sample a batch of experiences from the buffer\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Network Update\n",
    "\n",
    "if `soft` then:\n",
    "for each parameter \n",
    "* we take a fraction (`tau`) of the corresponding parameter from the `model`\n",
    "* we take a large fraction (`1-tau`) of the **current** parameter from the `target_model`\n",
    "\n",
    "else (`hard`) then:\n",
    "load the weights and biases of `model` into `target_model`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target_model(model, target_model, soft=False, tau=0.1):\n",
    "    \"\"\"\n",
    "    Update the target model with the online model's weights (main network).\n",
    "    Parameters:\n",
    "    - model: The online model.\n",
    "    - target_model: The target model.\n",
    "    - soft: If True, use soft update (tau) instead of hard update.\n",
    "    - tau: The interpolation factor for soft update.\n",
    "      \"\"\"\n",
    "    if soft:\n",
    "        target_model_state_dict = target_model.state_dict()\n",
    "        model_state_dict = model.state_dict()\n",
    "        for key in model_state_dict:\n",
    "            target_model_state_dict[key] = model_state_dict[key]*tau + target_model_state_dict[key]*(1-tau)\n",
    "        target_model.load_state_dict(target_model_state_dict)\n",
    "    else:\n",
    "        target_model.load_state_dict(model.state_dict())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Loss Function\n",
    "\n",
    "Computes the loss for the DQN Agent. Implements the **Bellman Backup** \n",
    "\n",
    "$$L = E\\bigg[Q(s, a) - (r + \\gamma \\ \\mathrm{max}_a Q'(s´,a´)))^2 \\bigg]$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN loss function and training loop\n",
    "def dqn_loss(model, target_model, batch, gamma):\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "    # Convert to tensors\n",
    "    states = torch.FloatTensor(states)\n",
    "    actions = torch.LongTensor(actions).unsqueeze(1)\n",
    "    rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "    next_states = torch.FloatTensor(next_states)\n",
    "    dones = torch.FloatTensor(dones).unsqueeze(1)\n",
    "\n",
    "    # Compute Q-values\n",
    "    # Q(s, a) = r + gamma * max_a' Q(s', a')\n",
    "    # where s' is the next state, a' is the action taken in the next state\n",
    "    # and r is the reward received\n",
    "    q_values = model(states).gather(1, actions)\n",
    "    next_q_values = target_model(next_states).max(1)[0].detach()\n",
    "    expected_q_values = rewards + (1 - dones) * gamma * next_q_values\n",
    "    # Compute the loss\n",
    "    loss = nn.MSELoss()(q_values, expected_q_values.unsqueeze(1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model\n",
    "\n",
    "Takes a batch of past experiences and calculates how wrong the current model predictions are (i.e the loss from `dqn_loss(...)`) and then updates the models weights to make its predictions better in future iterations\n",
    "\n",
    "* `model` is the online network meanwhile `target_model` is the frozen (or slowly) updated target network (the target Q-network). \n",
    "* `optimizer` is the PyTorch `Adam` that will update the `model`s weights\n",
    "* `replay_buffer` is the replaybuffer object from `ReplayBuffer` class\n",
    "* `gamma` is the discount factor used in the Q-learning update rule, determining the importance of future rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model, target_model, replay_buffer, optimizer, batch_size, gamma\n",
    "):\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return\n",
    "\n",
    "    batch = replay_buffer.sample(batch_size)\n",
    "    loss = dqn_loss(model, target_model, batch, gamma)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\epsilon$-greedy policy\n",
    "\n",
    "Implements the $\\epsilon$-greedy action selection strategy (in order to balance **exploration** and **exploitation**)\n",
    "\n",
    "* **Exploration**: If the `random.random()` is less than `epsilon`, the agent **explore** (not using its current knowledge)\n",
    "\n",
    "* **Exploitation**: If the `random.random()` is **not** less than `epsilon`, the agent **exploits** its current knowledge, using Q-values from the model with the current state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_greedy(state, model, epsilon, action_dim):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, action_dim - 1)\n",
    "    else:\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)  \n",
    "        with torch.no_grad():\n",
    "            q_values = model(state)\n",
    "        return q_values.argmax().item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function (training loop)\n",
    "\n",
    "Hyperparameters\n",
    "```input_size\n",
    "    hidden_size\n",
    "    output_size\n",
    "    num_episodes\n",
    "    batch_size\n",
    "    gamma\n",
    "    learning_rate\n",
    "    target_update_freq\n",
    "    replay_buffer_capacity\n",
    "```\n",
    "\n",
    "Continues on until `CartPole` environment-step gives `done=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Hyperparameters\n",
    "    env = gymnasium.make('CartPole-v1') # use gymnasium for the latest version\n",
    "    input_size = env.observation_space.shape[0]\n",
    "    hidden_size = 128\n",
    "    output_size = env.action_space.n\n",
    "    num_episodes = 1000\n",
    "    batch_size = 64\n",
    "    gamma = 0.99\n",
    "    learning_rate = 0.001\n",
    "    target_update_freq = 10\n",
    "    replay_buffer_capacity = 10000\n",
    "\n",
    "    # init the environment, model, optimizer, and replay buffer\n",
    "    model = DQN(input_size, hidden_size, output_size)\n",
    "    print(\"model\", model)\n",
    "    target_model = DQN(input_size, hidden_size, output_size)\n",
    "    target_model.load_state_dict(model.state_dict())\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    replay_buffer = ReplayBuffer(replay_buffer_capacity)\n",
    "\n",
    "    # main training loop\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            # using our greedy function\n",
    "            action = e_greedy(\n",
    "                state, model, epsilon=0.1, action_dim=output_size\n",
    "            )\n",
    "\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            # store experience in replay buffer\n",
    "            replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "            # update the state\n",
    "            state = next_state\n",
    "\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward}\")\n",
    "\n",
    "        # train the model\n",
    "        train_model(model, target_model, replay_buffer, optimizer, batch_size, gamma)\n",
    "\n",
    "        # update the target model every few (10) episodes\n",
    "        if episode % target_update_freq == 0:\n",
    "            update_target_model(model, target_model, soft=True, tau=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
